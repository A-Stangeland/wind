{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b5a5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab68f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torch.load(\"data/torch_dataset.pt\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ec194c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"y\"].isnan().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c89f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple_shared_sum_model.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Dataset\n",
    "# ----------------------------\n",
    "class WindAreaDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        X,\n",
    "        y,\n",
    "        x_mean=None,\n",
    "        x_std=None,\n",
    "        y_mean=None,\n",
    "        y_std=None,\n",
    "        normalize=True,\n",
    "        normalize_y=True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        X: (N, L, V) float tensor\n",
    "        y: (N,) float tensor\n",
    "        \"\"\"\n",
    "        self.X = X.float()\n",
    "        self.y = y.float()\n",
    "        self.normalize = normalize\n",
    "        self.normalize_y = normalize_y\n",
    "\n",
    "        # Stats (computed on the *training* partition and passed in for val)\n",
    "        self.x_mean = x_mean\n",
    "        self.x_std = x_std\n",
    "        self.y_mean = y_mean\n",
    "        self.y_std = y_std\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx]  # (L, V)\n",
    "        y = self.y[idx]  # scalar\n",
    "\n",
    "        if self.normalize and self.x_mean is not None and self.x_std is not None:\n",
    "            # normalize per variable across time+locations using train stats\n",
    "            x = (x - self.x_mean) / (self.x_std + 1e-6)\n",
    "\n",
    "        if self.normalize_y and self.y_mean is not None and self.y_std is not None:\n",
    "            y = (y - self.y_mean) / (self.y_std + 1e-6)\n",
    "\n",
    "        return x, y\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Model: shared per-location MLP + sum\n",
    "# ----------------------------\n",
    "class SharedPerLocationSum(nn.Module):\n",
    "    def __init__(self, in_dim=7, hidden=(64, 32), dropout=0.0, return_locals=False):\n",
    "        super().__init__()\n",
    "        h1, h2 = hidden\n",
    "        self.return_locals = return_locals\n",
    "        self.phi = nn.Sequential(\n",
    "            nn.Linear(in_dim, h1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(h1, h2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h2, 1),  # scalar contribution per location\n",
    "            # NOTE: if you want to enforce non-negativity of each contribution:\n",
    "            nn.Softplus(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, L, V)\n",
    "        returns:\n",
    "          y_hat: (B,) predicted total\n",
    "          (optionally) loc_contribs: (B, L)\n",
    "        \"\"\"\n",
    "        B, L, V = x.shape\n",
    "        z = x.view(B * L, V)  # flatten locations\n",
    "        contribs = self.phi(z).view(B, L)  # (B, L)\n",
    "        y_hat = contribs.sum(dim=1)  # (B,)\n",
    "        if self.return_locals:\n",
    "            return y_hat, contribs\n",
    "        return y_hat\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Training utility\n",
    "# ----------------------------\n",
    "def train_model(\n",
    "    data_path=\"data.pt\",\n",
    "    batch_size=512,\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-4,\n",
    "    epochs=10,\n",
    "    val_frac=0.1,\n",
    "    normalize_x=True,\n",
    "    normalize_y=True,\n",
    "    seed=42,\n",
    "    device=None,\n",
    "):\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # Load data\n",
    "    blob = torch.load(data_path)\n",
    "    X = blob[\"X\"].float()  # (N, L, V)\n",
    "    y = blob[\"y\"].float()  # (N,)\n",
    "\n",
    "    assert X.dim() == 3, f\"Expected X to be (N,L,V), got {tuple(X.shape)}\"\n",
    "    assert y.dim() == 1 and y.shape[0] == X.shape[0], \"y should be (N,) aligned with X\"\n",
    "\n",
    "    N, L, V = X.shape\n",
    "    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Train/val split (shuffle first)\n",
    "    n_val = int(N * val_frac)\n",
    "    n_train = N - n_val\n",
    "    train_ds_full = torch.utils.data.TensorDataset(X, y)\n",
    "    train_subset, val_subset = random_split(\n",
    "        train_ds_full, [n_train, n_val], generator=torch.Generator().manual_seed(seed)\n",
    "    )\n",
    "\n",
    "    # Compute normalization stats on train split ONLY\n",
    "    X_train = (\n",
    "        train_subset.dataset.tensors[0][train_subset.indices]\n",
    "        if hasattr(train_subset, \"indices\")\n",
    "        else X[:n_train]\n",
    "    )\n",
    "    y_train = (\n",
    "        train_subset.dataset.tensors[1][train_subset.indices]\n",
    "        if hasattr(train_subset, \"indices\")\n",
    "        else y[:n_train]\n",
    "    )\n",
    "\n",
    "    if normalize_x:\n",
    "        # mean/std over (time, location) for each variable (V,)\n",
    "        x_mean = X_train.mean(dim=(0, 1))  # (V,)\n",
    "        x_std = X_train.std(dim=(0, 1)) + 1e-6  # (V,)\n",
    "        # reshape to (1, V) for broadcasting on (L, V)\n",
    "        x_mean = x_mean.view(1, V)\n",
    "        x_std = x_std.view(1, V)\n",
    "    else:\n",
    "        x_mean = x_std = None\n",
    "\n",
    "    if normalize_y:\n",
    "        y_mean = y_train.mean()\n",
    "        y_std = y_train.std() + 1e-6\n",
    "    else:\n",
    "        y_mean = y_std = None\n",
    "\n",
    "    # Wrap datasets with normalization logic\n",
    "    def wrap_subset(subset):\n",
    "        X_sub = (\n",
    "            subset.dataset.tensors[0][subset.indices]\n",
    "            if hasattr(subset, \"indices\")\n",
    "            else subset.tensors[0]\n",
    "        )\n",
    "        y_sub = (\n",
    "            subset.dataset.tensors[1][subset.indices]\n",
    "            if hasattr(subset, \"indices\")\n",
    "            else subset.tensors[1]\n",
    "        )\n",
    "        return WindAreaDataset(\n",
    "            X_sub,\n",
    "            y_sub,\n",
    "            x_mean,\n",
    "            x_std,\n",
    "            y_mean,\n",
    "            y_std,\n",
    "            normalize=normalize_x,\n",
    "            normalize_y=normalize_y,\n",
    "        )\n",
    "\n",
    "    train_ds = wrap_subset(train_subset)\n",
    "    val_ds = wrap_subset(val_subset)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_ds, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_ds, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True\n",
    "    )\n",
    "\n",
    "    # Model, loss, optimizer\n",
    "    model = SharedPerLocationSum(in_dim=V, hidden=(64, 32), dropout=0.1).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    def denorm_y(t):\n",
    "        if not normalize_y:\n",
    "            return t\n",
    "        return t * y_std.to(t.device) + y_mean.to(t.device)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for xb, yb in train_loader:\n",
    "            xb = xb.to(device)  # (B, L, V)\n",
    "            yb = yb.to(device)  # (B,)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(xb)  # (B,)\n",
    "            loss = criterion(preds, yb)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * xb.size(0)\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader.dataset)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb = xb.to(device)\n",
    "                yb = yb.to(device)\n",
    "                preds = model(xb)\n",
    "                val_loss += criterion(preds, yb).item() * xb.size(0)\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader.dataset)\n",
    "\n",
    "        # Also report RMSE in original units for intuition\n",
    "        with torch.no_grad():\n",
    "            rmse = 0.0\n",
    "            n = 0\n",
    "            for xb, yb in val_loader:\n",
    "                xb = xb.to(device)\n",
    "                yb = yb.to(device)\n",
    "                preds = model(xb)\n",
    "                preds_den = denorm_y(preds)\n",
    "                yb_den = denorm_y(yb)\n",
    "                rmse += torch.sqrt(((preds_den - yb_den) ** 2).mean()).item() * xb.size(\n",
    "                    0\n",
    "                )\n",
    "                n += xb.size(0)\n",
    "            rmse /= max(n, 1)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch:02d} | train MSE: {avg_train_loss:.4f} | val MSE: {avg_val_loss:.4f} | val RMSE (orig units): {rmse:.3f}\"\n",
    "        )\n",
    "\n",
    "    return model, (x_mean, x_std, y_mean, y_std)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Adjust the path to your .pt file\n",
    "    model, stats = train_model(\n",
    "        data_path=\"your_data.pt\",\n",
    "        epochs=20,\n",
    "        batch_size=1024,\n",
    "        lr=2e-3,\n",
    "        weight_decay=1e-4,\n",
    "        normalize_x=True,\n",
    "        normalize_y=True,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wind (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
