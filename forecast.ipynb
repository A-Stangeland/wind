{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b5a5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import xarray as xr\n",
    "import plotly.express as px\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab68f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "da = xr.open_dataarray(\"data/weather_forecast.zarr\")\n",
    "print(da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c65d12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch + xarray/zarr training example for “shared-per-park → sum” model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "\n",
    "# -------- Dataset --------\n",
    "class TimeAsBatchDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Yields (X_t, y_t) for each time t.\n",
    "    X_t: (parks, forecast_step, feature) float32\n",
    "    y_t: scalar float32\n",
    "    Normalizes features over (time, park) so stats have shape (forecast_step, feature).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, x_da: xr.DataArray, y_total, times=None, normalize=True, norm_stats=None\n",
    "    ):\n",
    "        # x_da dims: time, forecast_step, park, feature (or as provided above)\n",
    "        expected_dims = (\"time\", \"forecast_step\", \"park\", \"feature\")\n",
    "        assert all(d in x_da.dims for d in expected_dims), (\n",
    "            f\"Expected dims {expected_dims}, got {x_da.dims}\"\n",
    "        )\n",
    "\n",
    "        # align and choose times\n",
    "        self.x_da = x_da\n",
    "        if hasattr(y_total, \"indexes\") or hasattr(y_total, \"coords\"):\n",
    "            # xarray DataArray/ Dataset or pandas Series with a time-like index\n",
    "            common_times = np.intersect1d(\n",
    "                np.asarray(x_da[\"time\"].values),\n",
    "                np.asarray(\n",
    "                    y_total[\"time\"].values\n",
    "                    if \"time\" in getattr(y_total, \"coords\", {})\n",
    "                    else y_total.index.values\n",
    "                ),\n",
    "            )\n",
    "            self.y_is_xr = True\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"y_total must be an xarray DataArray or pandas Series indexed by time.\"\n",
    "            )\n",
    "        if times is None:\n",
    "            times = common_times\n",
    "        self.times = np.asarray(times)\n",
    "\n",
    "        # compute or set normalization stats: mean/std over (time, park)\n",
    "        self.normalize = normalize\n",
    "        fs_dim, feat_dim = x_da.sizes[\"forecast_step\"], x_da.sizes[\"feature\"]\n",
    "        if norm_stats is not None:\n",
    "            self.mean_fs_feat, self.std_fs_feat = norm_stats\n",
    "        elif normalize:\n",
    "            # mean/std over time and park -> shapes (forecast_step, feature)\n",
    "            # (This may trigger a read; consider precomputing and passing norm_stats in production.)\n",
    "            mu = x_da.mean(dim=(\"time\", \"park\"), skipna=True)\n",
    "            sd = x_da.std(dim=(\"time\", \"park\"), skipna=True)\n",
    "            self.mean_fs_feat = mu.transpose(\"forecast_step\", \"feature\").values.astype(\n",
    "                np.float32\n",
    "            )\n",
    "            self.std_fs_feat = (\n",
    "                sd.transpose(\"forecast_step\", \"feature\").values.astype(np.float32)\n",
    "                + 1e-6\n",
    "            )\n",
    "        else:\n",
    "            self.mean_fs_feat = np.zeros((fs_dim, feat_dim), dtype=np.float32)\n",
    "            self.std_fs_feat = np.ones((fs_dim, feat_dim), dtype=np.float32)\n",
    "\n",
    "        # cache sizes\n",
    "        self.n_parks = x_da.sizes[\"park\"]\n",
    "        self.n_steps = x_da.sizes[\"forecast_step\"]\n",
    "        self.n_feat = x_da.sizes[\"feature\"]\n",
    "        self.y_total = y_total\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.times)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        t = self.times[idx]\n",
    "        # select one time → (forecast_step, park, feature)\n",
    "        xt = (\n",
    "            self.x_da.sel(time=t)\n",
    "            .transpose(\"park\", \"forecast_step\", \"feature\")\n",
    "            .values.astype(np.float32)\n",
    "        )  # (P,S,F)\n",
    "        # normalize broadcast over parks\n",
    "        if self.normalize:\n",
    "            xt = (xt - self.mean_fs_feat[None, :, :]) / self.std_fs_feat[None, :, :]\n",
    "\n",
    "        # replace NaNs if any\n",
    "        if np.isnan(xt).any():\n",
    "            xt = np.nan_to_num(xt, nan=0.0)\n",
    "\n",
    "        # target\n",
    "        if self.y_is_xr:\n",
    "            yt = float(self.y_total.sel(time=t).values)\n",
    "        else:\n",
    "            yt = float(self.y_total.loc[t])\n",
    "        return torch.from_numpy(xt), torch.tensor(yt, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# -------- Model: shared per-park φ, sum aggregation --------\n",
    "class PerParkMLP(nn.Module):\n",
    "    \"\"\"Shared small MLP mapping (S,F) → scalar per park.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, forecast_steps: int, n_features: int, hidden=256, hidden2=64, nonneg=True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        in_dim = forecast_steps * n_features\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, hidden2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden2, 1),\n",
    "        )\n",
    "        self.nonneg = nonneg\n",
    "\n",
    "    def forward(self, x):  # x: (B, P, S, F)\n",
    "        B, P, S, F = x.shape\n",
    "        z = x.reshape(B * P, S * F)\n",
    "        z = self.mlp(z)  # (B*P, 1)\n",
    "        if self.nonneg:\n",
    "            z = F.softplus(z)  # enforce non-negative per-park production\n",
    "        return z.view(B, P)  # (B, P)\n",
    "\n",
    "\n",
    "class SumOverParksModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Applies shared φ to each park, returns:\n",
    "      - total = sum over parks\n",
    "      - per_park = individual park scalars (useful for diagnostics)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, forecast_steps: int, n_features: int):\n",
    "        super().__init__()\n",
    "        self.phi = PerParkMLP(forecast_steps, n_features)\n",
    "\n",
    "    def forward(self, x):  # x: (B, P, S, F)\n",
    "        per_park = self.phi(x)  # (B, P)\n",
    "        total = per_park.sum(dim=1)  # (B,)\n",
    "        return total, per_park\n",
    "\n",
    "\n",
    "# -------- Example training loop --------\n",
    "def train_loop(\n",
    "    x_da: xr.DataArray,\n",
    "    y_total,\n",
    "    train_times,\n",
    "    val_times=None,\n",
    "    batch_size=64,\n",
    "    epochs=10,\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-4,\n",
    "    num_workers=0,\n",
    "    device=None,\n",
    "):\n",
    "    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    train_ds = TimeAsBatchDataset(x_da, y_total, times=train_times, normalize=True)\n",
    "    val_ds = (\n",
    "        TimeAsBatchDataset(\n",
    "            x_da,\n",
    "            y_total,\n",
    "            times=val_times,\n",
    "            normalize=True,\n",
    "            norm_stats=(train_ds.mean_fs_feat, train_ds.std_fs_feat),\n",
    "        )\n",
    "        if val_times is not None\n",
    "        else None\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    val_loader = (\n",
    "        DataLoader(\n",
    "            val_ds,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "        if val_ds is not None\n",
    "        else None\n",
    "    )\n",
    "\n",
    "    model = SumOverParksModel(\n",
    "        forecast_steps=x_da.sizes[\"forecast_step\"], n_features=x_da.sizes[\"feature\"]\n",
    "    ).to(device)\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        # --- train ---\n",
    "        model.train()\n",
    "        running = []\n",
    "        for xb, yb in train_loader:\n",
    "            xb = xb.to(device, non_blocking=True)  # (B,P,S,F)\n",
    "            yb = yb.to(device, non_blocking=True)  # (B,)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            yhat, _ = model(xb)\n",
    "            loss = loss_fn(yhat, yb)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            opt.step()\n",
    "            running.append(loss.item())\n",
    "\n",
    "        msg = f\"[{epoch:03d}] train MSE: {np.mean(running):.4f}\"\n",
    "\n",
    "        # --- validate ---\n",
    "        if val_loader is not None:\n",
    "            model.eval()\n",
    "            v_running, v_mae = [], []\n",
    "            with torch.no_grad():\n",
    "                for xb, yb in val_loader:\n",
    "                    xb = xb.to(device, non_blocking=True)\n",
    "                    yb = yb.to(device, non_blocking=True)\n",
    "                    yhat, _ = model(xb)\n",
    "                    v_running.append(loss_fn(yhat, yb).item())\n",
    "                    v_mae.append(torch.mean(torch.abs(yhat - yb)).item())\n",
    "            msg += (\n",
    "                f\" | val MSE: {np.mean(v_running):.4f} | val MAE: {np.mean(v_mae):.4f}\"\n",
    "            )\n",
    "        print(msg)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# ---------- Usage sketch ----------\n",
    "# x_da: your weather_features DataArray with dims (time, forecast_step, park, feature)\n",
    "# y_total: xarray DataArray (time,) with total production per bid area at each time\n",
    "\n",
    "# x_da = xr.open_zarr(\"path/to.zarr\")[\"weather_features\"]  # if not already loaded\n",
    "# y_total = xr.open_dataset(\"targets.zarr\")[\"total_power\"] # or any aligned (time,) series\n",
    "\n",
    "# times = np.asarray(x_da[\"time\"].values)\n",
    "# split_time = int(0.8 * len(times))\n",
    "# train_times, val_times = times[:split_time], times[split_time:]\n",
    "\n",
    "# model = train_loop(x_da, y_total, train_times, val_times, epochs=20, batch_size=128, lr=3e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f14f395",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a99786d",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_forecast = pl.scan_parquet(\"data/met_forecast.parquet\")\n",
    "weather_nowcast = pl.scan_parquet(\"data/met_nowcast.parquet\")\n",
    "windpower = pl.scan_parquet(\"data/wind_power_per_bidzone.parquet\").rename(\n",
    "    {\"__index_level_0__\": \"time\"}\n",
    ")\n",
    "windparks = pl.scan_csv(\"data/windparks_bidzone.csv\", try_parse_dates=True).filter(\n",
    "    pl.col(\"eic_code\") == pl.col(\"eic_code\").first().over(\"substation_name\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5390f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "windparks.select(pl.col(\"prod_start_new\").max()).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7f7be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bid_zone = \"ELSPOT NO3\"\n",
    "ensemble_member = 0\n",
    "bid_zone_weather = (\n",
    "    weather_forecast.join(windparks, left_on=\"sid\", right_on=\"substation_name\")\n",
    "    .filter(\n",
    "        pl.col(\"bidding_area\") == bid_zone,\n",
    "        pl.col(\"time_ref\") > pl.col(\"prod_start_new\"),\n",
    "    )\n",
    "    .select(\n",
    "        \"sid\",\n",
    "        # \"prod_start_new\",\n",
    "        \"time_ref\",\n",
    "        \"time\",\n",
    "        \"lt\",\n",
    "        \"operating_power_max\",\n",
    "        f\"ws10m_{ensemble_member:02d}\",\n",
    "        f\"t2m_{ensemble_member:02d}\",\n",
    "        f\"rh2m_{ensemble_member:02d}\",\n",
    "        f\"mslp_{ensemble_member:02d}\",\n",
    "        f\"g10m_{ensemble_member:02d}\",\n",
    "    )\n",
    ")\n",
    "bid_zone_weather.tail(10).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708bde09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "for name, data in bid_zone_weather.collect(engine=\"streaming\").group_by(\n",
    "    \"time_ref\", \"time\", maintain_order=False\n",
    "):\n",
    "    print(name)\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0645a6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def polars_to_xarray_dataarray(X: pl.LazyFrame, zarr_path: str) -> xr.DataArray:\n",
    "    # Define dimensions\n",
    "    features = [\n",
    "        \"lt\",\n",
    "        \"operating_power_max\",\n",
    "        \"ws10m_00\",\n",
    "        \"t2m_00\",\n",
    "        \"rh2m_00\",\n",
    "        \"mslp_00\",\n",
    "        \"g10m_00\",\n",
    "    ]\n",
    "\n",
    "    # Unique sorted values for dimensions\n",
    "    times = X.select(\"time\").unique().collect().to_series().sort()\n",
    "    forecast_steps = X.select(\"lt\").unique().collect().to_series().sort()\n",
    "    num_stations = X.select(pl.n_unique(\"sid\")).collect().item()\n",
    "    sids = list(range(num_stations))\n",
    "\n",
    "    # Mapping for fast lookup\n",
    "    times_idx = {t: i for i, t in enumerate(times)}\n",
    "    # forecast_idx = {t: i for i, t in enumerate(forecast_steps)}\n",
    "    num_features = len(features)\n",
    "\n",
    "    # Initialize dense array with NaNs\n",
    "    shape = (len(times_idx), len(forecast_steps), num_stations, num_features)\n",
    "    data = np.full(shape, np.nan, dtype=np.float32)\n",
    "\n",
    "    # Fill values\n",
    "    for (time_ref, forecast_step), group_data in X.collect(engine=\"streaming\").group_by(\n",
    "        \"time\", \"lt\", maintain_order=False\n",
    "    ):\n",
    "        i = times_idx[time_ref]\n",
    "        h = group_data.height\n",
    "        print(time_ref, forecast_step)\n",
    "        data[i, forecast_step, :h, :] = group_data.select(features).to_numpy()\n",
    "\n",
    "    # Create xarray DataArray\n",
    "    da = xr.DataArray(\n",
    "        data,\n",
    "        dims=[\"time\", \"forecast_step\", \"park\", \"feature\"],\n",
    "        coords={\n",
    "            \"time\": times,\n",
    "            \"forecast_step\": forecast_steps,\n",
    "            \"park\": sids,\n",
    "            \"feature\": features,\n",
    "        },\n",
    "        name=\"weather_features\",\n",
    "    )\n",
    "\n",
    "    # Save to zarr\n",
    "    da.to_dataset().to_zarr(zarr_path, mode=\"w\")\n",
    "    print(f\"Saved DataArray to {zarr_path}\")\n",
    "    return da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ac4d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = polars_to_xarray_dataarray(bid_zone_weather, \"data/weather_forecast.zarr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c7a5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class SharedMLP(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64), nn.ReLU(), nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x).squeeze(-1)  # (N,) output\n",
    "\n",
    "\n",
    "class SumModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.shared_mlp = SharedMLP(input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.shared_mlp(x)  # shape: (num_sids,)\n",
    "        return outputs.sum()  # sum across all parks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc95a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PowerDataset(Dataset):\n",
    "    def __init__(self, X_df: pl.DataFrame, y_df: pl.DataFrame):\n",
    "        self.feature_cols = [\n",
    "            \"lt\",\n",
    "            \"operating_power_max\",\n",
    "            \"ws10m_00\",\n",
    "            \"t2m_00\",\n",
    "            \"rh2m_00\",\n",
    "            \"mslp_00\",\n",
    "            \"g10m_00\",\n",
    "        ]\n",
    "        self.sids = sorted(X_df[\"sid\"].unique().to_list())\n",
    "        self.sid_to_idx = {sid: i for i, sid in enumerate(self.sids)}\n",
    "        self.num_sids = len(self.sids)\n",
    "        self.num_features = len(self.feature_cols)\n",
    "\n",
    "        # Create sorted list of (time_ref, lt) keys\n",
    "        self.keys = (\n",
    "            X_df.select([\"time_ref\", \"lt\"]).unique().sort([\"time_ref\", \"lt\"]).rows()\n",
    "        )\n",
    "\n",
    "        # Map from (time_ref, lt) to index\n",
    "        self.key_to_idx = {key: idx for idx, key in enumerate(self.keys)}\n",
    "        self.N = len(self.keys)\n",
    "\n",
    "        # Initialize the full input tensor\n",
    "        self.inputs = torch.zeros(\n",
    "            (self.N, self.num_sids, self.num_features), dtype=torch.float32\n",
    "        )\n",
    "\n",
    "        # Fill the tensor\n",
    "        for row in X_df.iter_rows(named=True):\n",
    "            time_ref, lt, sid = row[\"time_ref\"], row[\"lt\"], row[\"sid\"]\n",
    "            print(time_ref, lt, sid)\n",
    "            key = (time_ref, lt)\n",
    "            if key not in self.key_to_idx:\n",
    "                continue\n",
    "            i = self.key_to_idx[key]\n",
    "            j = self.sid_to_idx[sid]\n",
    "            features = [float(row[col]) for col in self.feature_cols]\n",
    "            self.inputs[i, j] = torch.tensor(features)\n",
    "\n",
    "        # Load targets\n",
    "        self.targets = torch.tensor(y_df[\"Power\"].to_numpy(), dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.N\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.targets[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc5c680",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, epochs=20, lr=1e-3):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for i, (X_batch, y_batch) in enumerate(dataloader):\n",
    "            print(f\"Batch {i}, dims={X_batch.shape}\")\n",
    "            optimizer.zero_grad()\n",
    "            # Only one sample per batch, unpack\n",
    "            preds = [model(x) for x in X_batch]  # list of scalars\n",
    "            preds = torch.stack(preds)\n",
    "            loss = loss_fn(preds, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(dataloader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a92ee9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your dataframes are X and y\n",
    "dataset = PowerDataset(X, y)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "model = SumModel(input_dim=7)\n",
    "train_model(model, dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f45fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00acec5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064fd80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = bid_zone_weather.collect()\n",
    "da = polars_to_xarray_dataarray(X, \"weather_forecast_array.zarr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bf0497",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_zarr(\"weather_forecast_array.zarr\")\n",
    "da = ds[\"weather_features\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wind (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
