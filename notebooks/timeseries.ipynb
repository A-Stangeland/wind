{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a4f1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ~/projects/wind/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e98ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "# import torch\n",
    "import plotly.express as px\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edff03e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_gaps(df: pl.LazyFrame) -> pl.LazyFrame:\n",
    "    times = df.select(\n",
    "        pl.datetime_range(pl.col(\"time\").min(), pl.col(\"time\").max(), \"1h\")\n",
    "    )\n",
    "    filled = times.join(df, on=\"time\", how=\"left\").fill_null(strategy=\"forward\")\n",
    "    return filled\n",
    "\n",
    "\n",
    "lookback = 48\n",
    "forecast_lead_time = 39\n",
    "forecast_window = 24\n",
    "bidding_areas = [\n",
    "    \"ELSPOT NO1\",\n",
    "    \"ELSPOT NO2\",\n",
    "    \"ELSPOT NO3\",\n",
    "    \"ELSPOT NO4\",\n",
    "]\n",
    "windpower = (\n",
    "    pl.scan_parquet(\"data/wind_power_per_bidzone.parquet\")\n",
    "    .rename({\"__index_level_0__\": \"time\"})\n",
    "    .filter(pl.col(\"time\") >= datetime(2021, 1, 1))\n",
    "    .sort(\"time\")\n",
    "    .pipe(fill_gaps)\n",
    "    .with_columns(\n",
    "        lookback_start=pl.col(\"time\").shift(lookback - 1),\n",
    "        window_start=pl.col(\"time\").shift(-forecast_lead_time),\n",
    "        window_stop=pl.col(\"time\").shift(-(forecast_lead_time + forecast_window - 1)),\n",
    "    )\n",
    ")\n",
    "capacity = (\n",
    "    windpower.select(pl.col(f\"ELSPOT NO{k}\").max() for k in range(1, 5))\n",
    "    .collect()\n",
    "    .to_numpy()[0]\n",
    ")\n",
    "windpower = windpower.with_columns(\n",
    "    pl.col(f\"ELSPOT NO{k}\").forward_fill() / capacity[k - 1] for k in range(1, 5)\n",
    ").collect()\n",
    "\n",
    "windpower.drop_nulls()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2202ed9",
   "metadata": {},
   "source": [
    "## Auto-Regressive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee41f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "bidding_area = \"ELSPOT NO3\"\n",
    "ts = windpower.filter(pl.col(\"time\").dt.year() == 2024).sort(\"time\").get_column(bidding_area).to_numpy()\n",
    "ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f502af6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "adf_test = adfuller(ts)\n",
    "# Output the results\n",
    "print('ADF Statistic: %f' % adf_test[0])\n",
    "print('p-value: %f' % adf_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0821adc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "import numpy as np\n",
    "ax = plot_acf(ts, lags=np.arange(39, 120))\n",
    "plt.gca().set_ylim(-0.1, 0.5)\n",
    "plot_pacf(ts, lags=np.arange(39, 120))\n",
    "plt.gca().set_ylim(-0.05, 0.05)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda3458d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "import seaborn as sns\n",
    "start = 1000\n",
    "n_train = 500\n",
    "n_val = 100\n",
    "t_train = np.arange(n_train)\n",
    "t_val = np.arange(n_train, n_train + n_val)\n",
    "train = ts[start:start + n_train]\n",
    "val = ts[start + n_train:start + n_train + n_val]\n",
    "ar_model = AutoReg(train, lags=2).fit()\n",
    "\n",
    "out = 'AIC: {0:0.3f}, HQIC: {1:0.3f}, BIC: {2:0.3f}'\n",
    "print(out.format(ar_model.aic, ar_model.hqic, ar_model.bic))\n",
    "\n",
    "pred = ar_model.get_prediction(end=n_train+100+n_val-1)\n",
    "f, ax = plt.subplots(nrows=1, ncols=1, figsize=(12, 4))\n",
    "sns.lineplot(x=t_val, y=val, marker='o', label='test', color='grey')\n",
    "sns.lineplot(x=t_train, y=train, marker='o', label='train')\n",
    "sns.lineplot(x=t_val, y=pred.se_mean, marker='o', label='pred')\n",
    "# ax.set_xlim([sample.timestamp.iloc[0], sample.timestamp.iloc[-1]])\n",
    "ax.set_title('Sample Time Series')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e7c21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = ar_model.get_prediction(start=n_train, end=n_train+n_val)\n",
    "res.se_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a83b615",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc as pm\n",
    "\n",
    "# Create an AR of order 2, with a constant term\n",
    "with pm.Model() as AR2:\n",
    "    # The first coefficient will be the constant term\n",
    "    coefs = pm.Normal(\"coefs\", 0, size=3)\n",
    "    # We need one init variable for each lag, hence size=3\n",
    "    init = pm.Normal.dist(5, size=2)\n",
    "    ar2 = pm.AR(\"ar2\", coefs, sigma=1.0, init_dist=init, constant=True, steps=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371034fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up a dictionary for the specification of our priors\n",
    "## We set up the dictionary to specify size of the AR coefficients in\n",
    "## case we want to vary the AR lags.\n",
    "priors = {\n",
    "    \"coefs\": {\"mu\": [10, 0.2], \"sigma\": [0.1, 0.1], \"size\": 2},\n",
    "    \"sigma\": 8,\n",
    "    \"init\": {\"mu\": 9, \"sigma\": 0.1, \"size\": 1},\n",
    "}\n",
    "\n",
    "## Initialise the model\n",
    "with pm.Model() as AR:\n",
    "    pass\n",
    "\n",
    "## Define the time interval for fitting the data\n",
    "t_data = list(range(len(ar1_data)))\n",
    "## Add the time interval as a mutable coordinate to the model to allow for future predictions\n",
    "AR.add_coord(\"obs_id\", t_data, mutable=True)\n",
    "\n",
    "with AR:\n",
    "    ## Data containers to enable prediction\n",
    "    t = pm.MutableData(\"t\", t_data, dims=\"obs_id\")\n",
    "    y = pm.MutableData(\"y\", ar1_data, dims=\"obs_id\")\n",
    "\n",
    "    # The first coefficient will be the constant term but we need to set priors for each coefficient in the AR process\n",
    "    coefs = pm.Normal(\"coefs\", priors[\"coefs\"][\"mu\"], priors[\"coefs\"][\"sigma\"])\n",
    "    sigma = pm.HalfNormal(\"sigma\", priors[\"sigma\"])\n",
    "    # We need one init variable for each lag, hence size is variable too\n",
    "    init = pm.Normal.dist(\n",
    "        priors[\"init\"][\"mu\"], priors[\"init\"][\"sigma\"], size=priors[\"init\"][\"size\"]\n",
    "    )\n",
    "    # Steps of the AR model minus the lags required\n",
    "    ar1 = pm.AR(\n",
    "        \"ar\",\n",
    "        coefs,\n",
    "        sigma=sigma,\n",
    "        init_dist=init,\n",
    "        constant=True,\n",
    "        steps=t.shape[0] - (priors[\"coefs\"][\"size\"] - 1),\n",
    "        dims=\"obs_id\",\n",
    "    )\n",
    "\n",
    "    # The Likelihood\n",
    "    outcome = pm.Normal(\"likelihood\", mu=ar1, sigma=sigma, observed=y, dims=\"obs_id\")\n",
    "    ## Sampling\n",
    "    idata_ar = pm.sample_prior_predictive()\n",
    "    idata_ar.extend(pm.sample(2000, random_seed=100, target_accept=0.95))\n",
    "    idata_ar.extend(pm.sample_posterior_predictive(idata_ar))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be424b90",
   "metadata": {},
   "source": [
    "## Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b06856",
   "metadata": {},
   "outputs": [],
   "source": [
    "times = (\n",
    "    windpower.filter(pl.col(\"time\").dt.hour() == 9)\n",
    "    .select(\"time\", \"lookback_start\", \"window_start\", \"window_stop\")\n",
    "    .drop_nulls()\n",
    ")\n",
    "N = len(times)\n",
    "X = torch.zeros((N, 4, lookback), dtype=torch.float32)\n",
    "y = torch.zeros((N, 4, forecast_window), dtype=torch.float32)\n",
    "for i, (time, lookback_start, window_start, window_stop) in enumerate(\n",
    "    times.iter_rows()\n",
    "):\n",
    "    print(time, lookback_start, window_start, window_stop)\n",
    "    X[i] = (\n",
    "        windpower.filter(\n",
    "            pl.col(\"time\") >= lookback_start,\n",
    "            pl.col(\"time\") <= time,\n",
    "        )\n",
    "        .select(bidding_areas)\n",
    "        .to_torch()\n",
    "        .t_()\n",
    "    )\n",
    "\n",
    "    y[i] = (\n",
    "        windpower.filter(\n",
    "            pl.col(\"time\") >= window_start,\n",
    "            pl.col(\"time\") <= window_stop,\n",
    "        )\n",
    "        .select(bidding_areas)\n",
    "        .to_torch()\n",
    "        .t_()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff991d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "\n",
    "val_frac = 0.25\n",
    "train_frac = 1 - val_frac\n",
    "n_train = int(N * train_frac)\n",
    "val_start = n_train + (forecast_lead_time + forecast_window - 1)\n",
    "\n",
    "X_train = X[:n_train].to(device)\n",
    "y_train = y[:n_train].to(device)\n",
    "\n",
    "X_val = X[val_start:].to(device)\n",
    "y_val = y[val_start:].to(device)\n",
    "n_val = X_val.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e7c325",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train, n_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca750c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_scale = torch.Tensor(capacity).unsqueeze(0).unsqueeze(-1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b82b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class LinearBlock(nn.Module):\n",
    "    def __init__(self, width, dropout):\n",
    "        super().__init__()\n",
    "        # self.norm = nn.LayerNorm(width)\n",
    "        self.linear = nn.Linear(width, width)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = F.sigmoid(self.linear(x))\n",
    "        h = self.dropout(h)\n",
    "        return h\n",
    "\n",
    "\n",
    "class TSModel(nn.Module):\n",
    "    def __init__(self, in_dim, out_dims, width=32, depth=4, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dims = out_dims\n",
    "        self.depth = depth\n",
    "\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Linear(in_dim, width),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Dropout(dropout),\n",
    "            # nn.Linear(in_dim, width),\n",
    "            # nn.Dropout(dropout),\n",
    "        )\n",
    "        if self.depth > 0:\n",
    "            self.layers = nn.Sequential(\n",
    "                *(LinearBlock(width, dropout) for _ in range(depth))\n",
    "            )\n",
    "\n",
    "        self.head = nn.Sequential(nn.Linear(width, out_dims), nn.Sigmoid())\n",
    "        # self.head = nn.Sequential(nn.Linear(width, out_dims))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        z = torch.flatten(x, 1)\n",
    "        z = self.stem(z)\n",
    "        if self.depth > 0:\n",
    "            z = self.layers(z)\n",
    "        z = self.head(z)\n",
    "        out = z.view(B, 4, -1)\n",
    "        return out\n",
    "\n",
    "\n",
    "class LogitRegressor(nn.Module):\n",
    "    def __init__(self, in_dim, out_dims):\n",
    "        super().__init__()\n",
    "        self.weights = nn.Sequential(nn.Linear(in_dim, out_dims), nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        z = torch.flatten(x, 1)\n",
    "        z = self.weights(z)\n",
    "        out = z.view(B, 4, -1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff68154",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 500\n",
    "lr = 1e-4\n",
    "patience = 20\n",
    "width = 128\n",
    "depth = 0\n",
    "dropout = 0.0\n",
    "\n",
    "writer = None\n",
    "os.makedirs(\"runs\", exist_ok=True)\n",
    "run_name = f\"{datetime.now().strftime('%Y%m%d-%H%M%S')}__ts\"\n",
    "writer = SummaryWriter(log_dir=os.path.join(\"runs\", run_name))\n",
    "\n",
    "model = TSModel(\n",
    "    4 * lookback, 4 * forecast_window, width=width, depth=depth, dropout=dropout\n",
    ").to(device)\n",
    "# model = LogitRegressor(\n",
    "#     4 * lookback, 4 * forecast_window).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "best_val = float(\"inf\")\n",
    "best_state = None\n",
    "pat_since_improve = 0\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()\n",
    "    train_sse = torch.tensor(0.0, device=device)\n",
    "    for i in range(0, n_train, batch_size):\n",
    "        xb, yb = X_train[i : i + batch_size], y_train[i : i + batch_size]\n",
    "\n",
    "        preds = model(xb)\n",
    "        loss = nn.functional.mse_loss(preds, yb)\n",
    "        # print(preds)\n",
    "        # print(yb)\n",
    "        # print(preds.shape, yb.shape)\n",
    "        # print(loss)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # nn.utils.clip_grad_norm_(model.parameters(), 20.0)\n",
    "        optimizer.step()\n",
    "        train_sse += loss.detach().float() * yb.numel()\n",
    "\n",
    "    avg_train_loss = train_sse.item() / n_train\n",
    "\n",
    "    model.eval()\n",
    "    val_sse = torch.tensor(0.0, device=device)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, n_val, batch_size):\n",
    "            xb, yb = X_val[i : i + batch_size], y_val[i : i + batch_size]\n",
    "            preds = model(xb)\n",
    "            val_sse += nn.functional.mse_loss(preds, yb, reduction=\"sum\")\n",
    "\n",
    "    avg_val_loss = val_sse.item() / n_val\n",
    "    val_rmse = torch.sqrt(val_sse / n_val).item()\n",
    "    print(\n",
    "        f\"Epoch {epoch:02d} | train MSE: {avg_train_loss:.4f} | val MSE: {avg_val_loss:.4f} | val RMSE: {val_rmse:.3f}\"\n",
    "    )\n",
    "    writer.add_scalar(\"Loss/train_MSE\", avg_train_loss, epoch)\n",
    "    writer.add_scalar(\"Loss/val_MSE\", avg_val_loss, epoch)\n",
    "    writer.add_scalar(\"Metrics/val_RMSE\", val_rmse, epoch)\n",
    "    writer.flush()\n",
    "\n",
    "    if avg_val_loss < best_val - 1e-4:\n",
    "        best_val = avg_val_loss\n",
    "        pat_since_improve = 0\n",
    "        best_state = {\n",
    "            k: v.detach().cpu().clone() for k, v in model.state_dict().items()\n",
    "        }\n",
    "    else:\n",
    "        pat_since_improve += 1\n",
    "        if pat_since_improve >= patience:\n",
    "            print(f\"Early stop at epoch {epoch} (best val={best_val:.4f})\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca89ad48",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    pred = model(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83950d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lead_times = [\n",
    "    str(lt) for lt in range(forecast_lead_time, forecast_lead_time + forecast_window)\n",
    "]\n",
    "df_pred = (\n",
    "    pl.concat(\n",
    "        [\n",
    "            pl.DataFrame(pred[:, k, :].cpu(), lead_times).with_columns(\n",
    "                bidding_area=pl.lit(f\"ELSPOT NO{k + 1}\"),\n",
    "                time_ref=times[\"time\"][n_train:],\n",
    "            )\n",
    "            for k in range(4)\n",
    "        ]\n",
    "    )\n",
    "    .unpivot(\n",
    "        index=[\"bidding_area\", \"time_ref\"], variable_name=\"lt\", value_name=\"y_pred\"\n",
    "    )\n",
    "    .with_columns(\n",
    "        lt=pl.col(\"lt\").cast(int), time=pl.col(\"time_ref\") + pl.duration(hours=\"lt\")\n",
    "    )\n",
    ")\n",
    "df_true = (\n",
    "    pl.concat(\n",
    "        [\n",
    "            pl.DataFrame(y_val[:, k, :].cpu(), lead_times).with_columns(\n",
    "                bidding_area=pl.lit(f\"ELSPOT NO{k + 1}\"),\n",
    "                time_ref=times[\"time\"][n_train:],\n",
    "            )\n",
    "            for k in range(4)\n",
    "        ]\n",
    "    )\n",
    "    .unpivot(\n",
    "        index=[\"bidding_area\", \"time_ref\"], variable_name=\"lt\", value_name=\"y_true\"\n",
    "    )\n",
    "    .with_columns(\n",
    "        lt=pl.col(\"lt\").cast(int), time=pl.col(\"time_ref\") + pl.duration(hours=\"lt\")\n",
    "    )\n",
    ")\n",
    "\n",
    "df = df_true.join(df_pred, on=[\"time_ref\", \"time\", \"lt\", \"bidding_area\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b586f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bidding_area = \"ELSPOT NO3\"\n",
    "px.line(\n",
    "    df.filter(pl.col(\"bidding_area\") == bidding_area)\n",
    "    .unpivot(index=[\"time_ref\", \"time\", \"lt\", \"bidding_area\"])\n",
    "    .sort(\"time\"),\n",
    "    \"time\",\n",
    "    \"value\",\n",
    "    color=\"variable\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338a59a1",
   "metadata": {},
   "source": [
    "## Bayesian Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90131ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pytensor\n",
    "import pytensor.tensor as pt\n",
    "import pymc as pm\n",
    "\n",
    "floatX = pytensor.config.floatX\n",
    "RANDOM_SEED = 9927\n",
    "rng = np.random.default_rng(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05a1fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "bidding_area_index = 0\n",
    "X_train = X_train[:, bidding_area_index, :].cpu().numpy()\n",
    "y_train = y_train[:, bidding_area_index, :].cpu().numpy()\n",
    "X_val = X_val[:, bidding_area_index, :].cpu().numpy()\n",
    "y_val = y_val[:, bidding_area_index, :].cpu().numpy()\n",
    "\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bbfaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return pt.switch(pt.lt(x, 0), 0, x)\n",
    "\n",
    "\n",
    "def construct_nn(batch_size=50):\n",
    "    n_hidden = 5\n",
    "\n",
    "    # Initialize random weights between each layer\n",
    "    init_1 = rng.standard_normal(size=(X_train.shape[1], n_hidden)).astype(floatX)\n",
    "    init_2 = rng.standard_normal(size=(n_hidden, n_hidden)).astype(floatX)\n",
    "    init_out_mu = rng.standard_normal(size=(n_hidden, y_train.shape[1])).astype(floatX)\n",
    "    init_out_sig = rng.standard_normal(size=(n_hidden, y_train.shape[1])).astype(floatX)\n",
    "\n",
    "    coords = {\n",
    "        \"hidden_layer_1\": np.arange(n_hidden),\n",
    "        \"hidden_layer_2\": np.arange(n_hidden),\n",
    "        \"train_cols\": np.arange(X_train.shape[1]),\n",
    "        \"obs_id\": np.arange(X_train.shape[0]),\n",
    "        \"target_cols\": np.arange(y_train.shape[1]),\n",
    "    }\n",
    "\n",
    "    with pm.Model(coords=coords) as neural_network:\n",
    "        # Define data variables using minibatches\n",
    "        X_data = pm.Data(\"X_data\", X_train, dims=(\"obs_id\", \"train_cols\"))\n",
    "        Y_data = pm.Data(\"Y_data\", y_train, dims=(\"obs_id\", \"target_cols\"))\n",
    "\n",
    "        # Define minibatch variables\n",
    "        ann_input, ann_output = pm.Minibatch(X_data, Y_data, batch_size=batch_size)\n",
    "\n",
    "        # Weights from input to hidden layer\n",
    "        weights_in_1 = pm.Normal(\n",
    "            \"w_in_1\", 0, sigma=1, initval=init_1, dims=(\"train_cols\", \"hidden_layer_1\")\n",
    "        )\n",
    "\n",
    "        # Weights from 1st to 2nd layer\n",
    "        weights_1_2 = pm.Normal(\n",
    "            \"w_1_2\",\n",
    "            0,\n",
    "            sigma=1,\n",
    "            initval=init_2,\n",
    "            dims=(\"hidden_layer_1\", \"hidden_layer_2\"),\n",
    "        )\n",
    "\n",
    "        # Weights from hidden layer to output\n",
    "        weights_2_out_mu = pm.Normal(\n",
    "            \"w_2_out_mu\",\n",
    "            0,\n",
    "            sigma=1,\n",
    "            initval=init_out_mu,\n",
    "            dims=(\"hidden_layer_2\", \"target_cols\"),\n",
    "        )\n",
    "        weights_2_out_sig = pm.Normal(\n",
    "            \"w_2_out_sig\",\n",
    "            0,\n",
    "            sigma=1,\n",
    "            initval=init_out_sig,\n",
    "            dims=(\"hidden_layer_2\", \"target_cols\"),\n",
    "        )\n",
    "\n",
    "        # Build neural-network using tanh activation function\n",
    "        act_1 = pt.sigmoid(pt.dot(ann_input, weights_in_1))\n",
    "        act_2 = pt.sigmoid(pt.dot(act_1, weights_1_2))\n",
    "        act_out_mu = pt.sigmoid(pt.dot(act_2, weights_2_out_mu))\n",
    "        act_out_sig = pt.exp(pt.dot(act_2, weights_2_out_sig))\n",
    "        out_sig = pm.HalfNormal(\"out_sig\", 0, 0.1, dims=\"target_cols\")\n",
    "\n",
    "        # Binary classification -> Bernoulli likelihood\n",
    "        out = pm.Normal(\n",
    "            \"out\",\n",
    "            act_out_mu,\n",
    "            act_out_sig,\n",
    "            observed=ann_output,\n",
    "            total_size=X_train.shape[0],  # IMPORTANT for minibatches\n",
    "        )\n",
    "    return neural_network\n",
    "\n",
    "\n",
    "# Create the neural network model\n",
    "neural_network = construct_nn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ab5956",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "with neural_network:\n",
    "    approx = pm.fit(n=10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6737367a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(approx.hist, alpha=0.3)\n",
    "plt.ylabel(\"ELBO\")\n",
    "plt.xlabel(\"iteration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d8e953",
   "metadata": {},
   "outputs": [],
   "source": [
    "trace = approx.sample(draws=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e94930",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_posterior_predictive(X_test, Y_test, trace, n_hidden=5):\n",
    "    coords = {\n",
    "        \"hidden_layer_1\": np.arange(n_hidden),\n",
    "        \"hidden_layer_2\": np.arange(n_hidden),\n",
    "        \"train_cols\": np.arange(X_test.shape[1]),\n",
    "        \"obs_id\": np.arange(X_test.shape[0]),\n",
    "    }\n",
    "    with pm.Model(coords=coords):\n",
    "        ann_input = X_test\n",
    "        ann_output = Y_test\n",
    "\n",
    "        weights_in_1 = pm.Flat(\"w_in_1\", dims=(\"train_cols\", \"hidden_layer_1\"))\n",
    "        weights_1_2 = pm.Flat(\"w_1_2\", dims=(\"hidden_layer_1\", \"hidden_layer_2\"))\n",
    "        weights_2_out = pm.Flat(\"w_2_out\", dims=\"hidden_layer_2\")\n",
    "\n",
    "        # Build neural-network using tanh activation function\n",
    "        act_1 = pm.math.tanh(pm.math.dot(ann_input, weights_in_1))\n",
    "        act_2 = pm.math.tanh(pm.math.dot(act_1, weights_1_2))\n",
    "        act_out = pm.math.sigmoid(pm.math.dot(act_2, weights_2_out))\n",
    "\n",
    "        # Binary classification -> Bernoulli likelihood\n",
    "        out = pm.Bernoulli(\"out\", act_out, observed=ann_output)\n",
    "        return pm.sample_posterior_predictive(trace)\n",
    "\n",
    "\n",
    "ppc = sample_posterior_predictive(X_test, Y_test, trace)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wind",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
